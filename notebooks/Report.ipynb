{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a0ed44",
   "metadata": {},
   "source": [
    "# Snake\n",
    "\n",
    "This notebook demonstrates the iterative process taken to develop a model capable of playing snake quite effectively.\n",
    "\n",
    "I am currently training an even better model than the one that can be seen at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3352d",
   "metadata": {},
   "source": [
    "## Instructions to run\n",
    "\n",
    "Feel free to run all cells in order, there is no model training that will occur by doing this. \n",
    "\n",
    "Cells responsible for model training are commented out for the purpose of observing the code for interested individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994df977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05033e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from definitions import ROOT\n",
    "\n",
    "from snake.environment.game_environment import (\n",
    "    BaseSnakeEnv, \n",
    ")\n",
    "from callbacks.callbacks import ScoreLoggerCallback\n",
    "from evaluation.evaluation import get_performance, produce_plots, watch_agent_play\n",
    "from snake.visualisation.state_visualisation import visualise_game_state\n",
    "\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d537d6",
   "metadata": {},
   "source": [
    "## Explaining the environment\n",
    "\n",
    "Stable baselines 3 (SB3) allows users to define a custom environment. Please see the class implementation in src/snake/environment/game_environment for more details. I chose to build my own environment because it allowed me the most flexibility.\n",
    "\n",
    "\n",
    "The state space seen by the snake is a 1-d array of length 100 representing a 10x10 grid. Each value is 0,1,2,3\n",
    "\n",
    "0 - empty\n",
    "\n",
    "1 - food\n",
    "\n",
    "2 - tail\n",
    "\n",
    "3 - head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c199846",
   "metadata": {},
   "source": [
    "### Run the below cell to see the environment for yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BaseSnakeEnv()\n",
    "env.reset()\n",
    "\n",
    "visualise_game_state(env.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4526ae11",
   "metadata": {},
   "source": [
    "## First benchmark\n",
    "\n",
    "This first model uses the SB3 default DQN multi-layer perceptron model.\n",
    "\n",
    "There is an input layer with 400 neurons. Each feature is a binary variable representing the presence or absence of each value 0,1,2,3 for each coordinate location.\n",
    "\n",
    "There are two fully-connected hidden layers with 64 neurons each and have ReLU activation. \n",
    "\n",
    "The final layer is 3 neurons that represent the three actions.\n",
    "\n",
    "\n",
    "The reward function is as follows:\n",
    "\n",
    "$$\n",
    "R(a, s) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if food is eaten in state } s \\text{ after action } a, \\\\\n",
    "-1, & \\text{if snake hits wall / own tail in state} s \\text{ after action } a, \\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "i train with 100_000 environment interactions to get a benchmark with about 5 minutes of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587183b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = BaseSnakeEnv()\n",
    "\n",
    "#model_dqn = DQN(\"MlpPolicy\", env=env)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "#score_logger = ScoreLoggerCallback()\n",
    "#model_dqn.learn(total_timesteps=100000, callback=score_logger)\n",
    "#metrics_dqn_first_benchmark = score_logger.get_metrics()\n",
    "#print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn = DQN.load(os.path.join(ROOT, \"../models/dqn_first_benchmark\"))\n",
    "metrics_dqn_first_benchmark = pd.read_csv(os.path.join(ROOT, \"../data/training_metrics/base_dqn.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interrupt kernel to stop rendering. Otherwise, rendering will stop after num_timesteps actions.\n",
    "watch_agent_play(model = model_dqn, env = BaseSnakeEnv(), num_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a7d42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "produce_plots(metrics_dqn_first_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f1409",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "This agent is pretty rubbish at finding food, but actually quite good at avoiding walls which can be seen from the average game length and average score. \n",
    "\n",
    "This example demonstrates the setup facilitates learning for an agent and acts as a foundation to build on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2cf81",
   "metadata": {},
   "source": [
    "## Taking advantage of rotational symmetry\n",
    "\n",
    "Observe the below states. Hopefully you aggree that they are actually the same, or at least the same logic should be applied to a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17235f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_board(info):\n",
    "    state = [0] * 100\n",
    "    \n",
    "    for i in info[:-1]:\n",
    "        state[i] = 2\n",
    "    state[info[-2]] = 3\n",
    "    state[info[-1]] = 1\n",
    "        \n",
    "    return state\n",
    "\n",
    "state1 = [0, 1, 2, 5]\n",
    "state2 = [9, 19, 29, 59]\n",
    "state3 = [99, 98, 97,94]\n",
    "state4 = [90, 80, 70, 40]\n",
    "\n",
    "visualise_game_state(fill_board(state1))\n",
    "visualise_game_state(fill_board(state2))\n",
    "visualise_game_state(fill_board(state3))\n",
    "visualise_game_state(fill_board(state4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b986b9",
   "metadata": {},
   "source": [
    "In this section I take advantage of this rotational symetry by rotating the state representation so the agent is alwyas moving upwards. Observing the enviornment would be impossible for humans but allows the agent to process rotationally symmetric states in the same logic.\n",
    "\n",
    "Using the same training structure as in the first benchmark, let's observe the difference in resutls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aab53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake.environment.game_environment import SnakeEnvRotatedState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = SnakeEnvRotatedState()\n",
    "\n",
    "#model_dqn_rotated_state = DQN(\"MlpPolicy\", env=env)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "#score_logger = ScoreLoggerCallback()\n",
    "#model_dqn_rotated_state.learn(total_timesteps=100000, callback=score_logger)\n",
    "#metrics_dqn_rotated_state = score_logger.get_metrics()\n",
    "\n",
    "#print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bec209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn_rotated_state = DQN.load(os.path.join(ROOT, \"../models/dqn_rotated_state\"))\n",
    "metrics_dqn_rotated_state = pd.read_csv(os.path.join(ROOT, \"../data/training_metrics/rotated_state_dqn.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â interrupt kernel to stop rendering, ohterwise rendering will end after 100 steps\n",
    "watch_agent_play(model_dqn_rotated_state, env = SnakeEnvRotatedState(), num_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(metrics_dqn_rotated_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8ad8b",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "Observing the average score plot, the an average score of approx 1.2 is acheived in about 25_000 training steps, 1/4 the time taken from the first benchmark. This is representative of the efficiency gain from this rotation. \n",
    "\n",
    "I will leave the state space representation the same from now on but wanted to demonstrate the gains that can be made by thinking about each part of the RL framework and how they can be optimised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed21c7",
   "metadata": {},
   "source": [
    "## Reward Engineering 1\n",
    "\n",
    "Right now, the agent is very poor at actually finding food. This is a common problem in RL problems where the reward comes after a sometimes long sequence of steps. Think of a terminator tasked with protecting John Connor, Arnold Schwarzenegger doesn't know until the end of the movie that this task was a success. \n",
    "\n",
    "To tackle this problem in my setting, I first introduce a smaller reward for making movements towards food.\n",
    "\n",
    "$$\n",
    "R(a, s) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if food is eaten in state } s \\text{ after action } a, \\\\\n",
    "-1, & \\text{if snake hits wall / own tail in state} s \\text{ after action } a, \\\\\n",
    "0.2, & \\text{if snake moved towards food after action } a, \\\\\n",
    "-0.2, & \\text{if snake moved away from food after action } a, \\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "I also use the newly rotated states and compare now to the more successful model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9841ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake.environment.game_environment import SnakeEnvRandS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e19566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = SnakeEnvRandS()\n",
    "\n",
    "#model_dqn_new_reward = DQN(\"MlpPolicy\", env=env)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "#score_logger = ScoreLoggerCallback()\n",
    "#model_dqn_new_reward.learn(total_timesteps=100000, callback=score_logger)\n",
    "#metrics_dqn_new_reward = score_logger.get_metrics()\n",
    "\n",
    "#print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec89802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn_new_reward = DQN.load(os.path.join(ROOT, \"../models/dqn_new_reward\"))\n",
    "metrics_dqn_new_reward = pd.read_csv(os.path.join(ROOT, \"../data/training_metrics/new_reward_dqn.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beaf4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(metrics_dqn_new_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d79aa",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "Observe two key things here\n",
    "1. Higher average score\n",
    "2. Shorter average games\n",
    "\n",
    "So the agent is finding food far more reliably now. However, consider how this agent might behave with a very long tail, in this situation it woul dbe better to move towards food slowly and safely rather than directly. Although, this is a good start for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7d58e",
   "metadata": {},
   "source": [
    "## Bias reduction\n",
    "\n",
    "In my currecnt approach the game is always reset to the same position. This leads to a high proportion of training data coming from the same state observations, leading to a model that is over trained on a small subset of states. Furthermore, there is an overly high proportion of training data where the snake is quite short. \n",
    "\n",
    "To combat this I implemented a random reset method so that, during training, the agent is reset to a random position. \n",
    "\n",
    "**Coil reset** - pick a random spot and a snake length then take a random walk towards the tail until max length  is reached or the tail is trapped.\n",
    "\n",
    "**Line reset** - pick a random snake length and draw a straight line in some direction of that length.\n",
    "\n",
    "Visualise the two below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73a3032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snake.environment.game_environment import SnakeEnvLineReset, SnakeEnvCoilReset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnvCoilReset()\n",
    "\n",
    "np.random.seed(41)\n",
    "state, _ = env.reset()\n",
    "\n",
    "visualise_game_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c666b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeEnvLineReset()\n",
    "\n",
    "np.random.seed(41)\n",
    "state, _ = env.reset()\n",
    "\n",
    "visualise_game_state(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bde4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = SnakeEnvCoilReset()\n",
    "\n",
    "#model_dqn_coil_reset = DQN(\"MlpPolicy\", env=env)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "#score_logger = ScoreLoggerCallback()\n",
    "#model_dqn_coil_reset.learn(total_timesteps=100000, callback=score_logger)\n",
    "#metrics_dqn_coil_reset = score_logger.get_metrics()\n",
    "\n",
    "#print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d86598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = SnakeEnvLineReset()\n",
    "\n",
    "#model_dqn_line_reset = DQN(\"MlpPolicy\", env=env)\n",
    "\n",
    "\n",
    "#start_time = time.time()\n",
    "\n",
    "#score_logger = ScoreLoggerCallback()\n",
    "#model_dqn_line_reset.learn(total_timesteps=100000, callback=score_logger)\n",
    "#metrics_dqn_line_reset = score_logger.get_metrics()\n",
    "\n",
    "#print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn_coil_reset = DQN.load(os.path.join(ROOT, \"../models/dqn_coil_reset\"))\n",
    "metrics_dqn_coil_reset = pd.read_csv(os.path.join(ROOT, \"../data/training_metrics/coil_reset_dqn.csv\"))\n",
    "\n",
    "model_dqn_line_reset = DQN.load(os.path.join(ROOT, \"../models/dqn_line_reset\"))\n",
    "metrics_dqn_line_reset = pd.read_csv(os.path.join(ROOT, \"../data/training_metrics/line_reset_dqn.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8beeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(metrics_dqn_coil_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a282ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "produce_plots(metrics_dqn_line_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b6c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_performance = get_performance(model = model_dqn_line_reset, env = SnakeEnvLineReset(max_reset_length=3), seed=42)\n",
    "\n",
    "coil_performance = get_performance(model = model_dqn_coil_reset, env = SnakeEnvLineReset(max_reset_length=3), seed=42)\n",
    "\n",
    "benchmarked_performance = get_performance(model = model_dqn_new_reward, env = SnakeEnvLineReset(max_reset_length=3), seed = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb3bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Benchmark:\", np.mean(benchmarked_performance))\n",
    "print(\"Line Reset:\", np.mean(line_performance))\n",
    "print(\"Coil Reset:\", np.mean(coil_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd33d3",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "The new methods perform quite similarly to the model that resets the agent to the same point every time. However, I think with a longer training cycle the differences woudl become more apparent. \n",
    "\n",
    "For now I will use the coil reset method, this is because I think it will scale better to have a longer snake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snake)",
   "language": "python",
   "name": "snake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
